{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93babe8b-5484-4cde-8474-4bbf0ff62536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "file_path = r'D:\\GUVI_Projects\\My_Projects\\Crimes_-_2001_to_Present.csv'\n",
    "save_path = r'D:\\GUVI_Projects\\My_Projects'\n",
    "\n",
    "# Load dataset with low memory\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(file_path, low_memory=True)\n",
    "\n",
    "# Basic inspection\n",
    "print(\"\\nDataset loaded successfully!\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nBasic Information:\")\n",
    "df.info()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check dataset dimensions\n",
    "print(\"\\nDataset dimensions:\", df.shape)\n",
    "\n",
    "# Save column names to a text file\n",
    "columns_file = save_path + '\\\\columns_list.txt'\n",
    "print(\"\\nSaving column names to:\", columns_file)\n",
    "with open(columns_file, 'w') as f:\n",
    "    for column in df.columns:\n",
    "        f.write(column + '\\n')\n",
    "\n",
    "print(\"Column names saved successfully!\")\n",
    "\n",
    "# Display summary statistics for numeric columns\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a3e2d1-e4c4-4e37-87de-ed877d910798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "\n",
    "    def load_data(self):\n",
    "        print(\"Loading dataset...\")\n",
    "        self.df = pd.read_csv(self.file_path, low_memory=True)\n",
    "        print(\"Dataset loaded successfully!\")\n",
    "\n",
    "    def inspect_data(self):\n",
    "        print(\"\\nBasic Information:\")\n",
    "        self.df.info()\n",
    "        print(\"\\nFirst 5 rows of the dataset:\")\n",
    "        print(self.df.head())\n",
    "        print(\"\\nMissing values in each column:\")\n",
    "        print(self.df.isnull().sum())\n",
    "        print(\"\\nDataset dimensions:\", self.df.shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_memory(df):\n",
    "        for col in df.select_dtypes(include=['float64']):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        for col in df.select_dtypes(include=['int64']):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        for col in df.select_dtypes(include=['object']):\n",
    "            df[col] = df[col].astype('category')\n",
    "        return df\n",
    "\n",
    "    def handle_missing_values(self):\n",
    "        # Fill missing numeric columns with 0\n",
    "        numeric_cols = self.df.select_dtypes(include=['float', 'int']).columns\n",
    "        self.df[numeric_cols] = self.df[numeric_cols].fillna(0)\n",
    "\n",
    "        # Fill missing object columns with 'Unknown'\n",
    "        object_cols = self.df.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in object_cols:\n",
    "            if self.df[col].dtype.name == 'category':\n",
    "                self.df[col] = self.df[col].cat.add_categories(['Unknown']).fillna('Unknown')\n",
    "            else:\n",
    "                self.df[col] = self.df[col].fillna('Unknown')\n",
    "\n",
    "        # Fill missing boolean columns with False\n",
    "        bool_cols = self.df.select_dtypes(include=['bool']).columns\n",
    "        self.df[bool_cols] = self.df[bool_cols].fillna(False)\n",
    "\n",
    "    def handle_dates(self):\n",
    "        # Convert date columns to datetime\n",
    "        date_columns = ['Date', 'Updated On']\n",
    "        for col in date_columns:\n",
    "            self.df[col] = pd.to_datetime(self.df[col], errors='coerce')\n",
    "\n",
    "        # Fill missing dates with a placeholder\n",
    "        self.df[date_columns] = self.df[date_columns].fillna(pd.Timestamp('2000-01-01'))\n",
    "\n",
    "    def drop_unnecessary_columns(self):\n",
    "        # Drop X Coordinate and Y Coordinate columns\n",
    "        columns_to_drop = ['X Coordinate', 'Y Coordinate']\n",
    "        self.df = self.df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    def save_cleaned_data(self, save_path):\n",
    "        cleaned_file = save_path + '\\\\cleaned_crime.csv'\n",
    "        print(\"\\nSaving cleaned data to:\", cleaned_file)\n",
    "        self.df.to_csv(cleaned_file, index=False)\n",
    "        print(\"Cleaned data saved successfully!\")\n",
    "\n",
    "    def save_column_mappings(self, save_path):\n",
    "        csv_file = save_path + '\\\\cleaned_mappings_crime.csv'\n",
    "        print(\"\\nSaving column mappings to:\", csv_file)\n",
    "        mappings = []\n",
    "        for column in self.df.columns:\n",
    "            unique_values = self.df[column].cat.categories if self.df[column].dtype.name == 'category' else self.df[column].unique()\n",
    "            mappings.append({\"Column\": column, \"Unique Values\": list(unique_values)})\n",
    "        mappings_df = pd.DataFrame(mappings)\n",
    "        mappings_df.to_csv(csv_file, index=False)\n",
    "        print(\"Column mappings saved successfully!\")\n",
    "\n",
    "    def process_data(self, save_path):\n",
    "        self.load_data()\n",
    "        self.df = self.reduce_memory(self.df)\n",
    "        self.handle_missing_values()\n",
    "        self.handle_dates()\n",
    "        self.drop_unnecessary_columns()\n",
    "        self.save_cleaned_data(save_path)\n",
    "        self.save_column_mappings(save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r'D:\\GUVI_Projects\\My_Projects\\Crimes_-_2001_to_Present.csv'\n",
    "    save_path = r'D:\\GUVI_Projects\\My_Projects'\n",
    "\n",
    "    processor = DataProcessor(file_path)\n",
    "    processor.process_data(save_path)\n",
    "    print(\"\\nData processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faef2b3-39c1-4fc9-8b06-e3964246e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, data_file_path, mappings_file_path):\n",
    "        self.data_file_path = data_file_path\n",
    "        self.mappings_file_path = mappings_file_path\n",
    "        self.df = None\n",
    "        self.scaler = None\n",
    "        self.label_encoders = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_memory(df):\n",
    "        logging.info(\"Reducing memory usage...\")\n",
    "        start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "        logging.info(f\"Initial memory usage: {start_mem:.2f} MB\")\n",
    "\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "            if col_type in ['int64', 'float64']:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if col_type == 'int64':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                elif col_type == 'float64':\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "        logging.info(f\"Reduced memory usage: {end_mem:.2f} MB ({((start_mem - end_mem) / start_mem) * 100:.1f}% reduction)\")\n",
    "        return df\n",
    "\n",
    "    def load_data(self):\n",
    "        logging.info(\"Loading dataset using Polars...\")\n",
    "        try:\n",
    "            polars_df = pl.read_csv(self.data_file_path)\n",
    "            self.df = polars_df.to_pandas()\n",
    "            logging.info(f\"Dataset loaded successfully with shape: {self.df.shape}\")\n",
    "\n",
    "            # Sample large dataset for development with stratified sampling\n",
    "            if self.df.shape[0] > 100000:\n",
    "                if 'Arrest' in self.df.columns:\n",
    "                    _, self.df = train_test_split(\n",
    "                        self.df, \n",
    "                        train_size=100000, \n",
    "                        stratify=self.df['Arrest'], \n",
    "                        random_state=42\n",
    "                    )\n",
    "                    logging.info(f\"Dataset stratified sampled to shape: {self.df.shape}\")\n",
    "                else:\n",
    "                    logging.warning(\"Stratified sampling skipped as 'Arrest' column is not present. Performing random sampling.\")\n",
    "                    self.df = self.df.sample(n=100000, random_state=42)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clean_unknown_values(self):\n",
    "        logging.info(\"Cleaning unknown values in the dataset...\")\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtype == 'object':\n",
    "                self.df[col] = self.df[col].fillna('Unknown').astype(str)\n",
    "            else:\n",
    "                self.df[col] = self.df[col].apply(pd.to_numeric, errors='coerce').fillna(0).astype(float)\n",
    "        logging.info(\"Unknown values cleaned and data types corrected.\")\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        logging.info(\"Preprocessing data...\")\n",
    "        self.clean_unknown_values()\n",
    "\n",
    "        # Convert categorical columns to numeric using LabelEncoder\n",
    "        categorical_cols = self.df.select_dtypes(include=['object', 'category']).columns\n",
    "        if len(categorical_cols) > 0:\n",
    "            logging.info(\"Converting categorical columns using LabelEncoder...\")\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                self.df[col] = le.fit_transform(self.df[col])\n",
    "                self.label_encoders[col] = le\n",
    "\n",
    "        # Standardize numeric columns\n",
    "        numeric_cols = self.df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            logging.info(\"Standardizing numeric columns...\")\n",
    "            self.scaler = StandardScaler()\n",
    "            self.df[numeric_cols] = self.scaler.fit_transform(self.df[numeric_cols])\n",
    "\n",
    "        logging.info(\"Data preprocessing complete.\")\n",
    "\n",
    "    def balance_data(self, target_col):\n",
    "        logging.info(\"Balancing data using SMOTE...\")\n",
    "\n",
    "        # Ensure target column is binary\n",
    "        if self.df[target_col].dtype == bool:\n",
    "            logging.info(f\"Encoding boolean target '{target_col}' to 0/1...\")\n",
    "            self.df[target_col] = self.df[target_col].astype(int)\n",
    "        elif self.df[target_col].dtype in ['float64', 'float32']:\n",
    "            logging.info(f\"Binarizing continuous target '{target_col}'...\")\n",
    "            median_value = self.df[target_col].median()\n",
    "            self.df[target_col] = (self.df[target_col] > median_value).astype(int)\n",
    "        elif self.df[target_col].nunique() > 2:\n",
    "            logging.info(f\"Converting multi-class target '{target_col}' to binary (simplified for demonstration)...\")\n",
    "            self.df[target_col] = (self.df[target_col] == self.df[target_col].mode()[0]).astype(int)\n",
    "\n",
    "        # Prepare features and target\n",
    "        X = self.df.drop(columns=[target_col])\n",
    "        y = self.df[target_col]\n",
    "\n",
    "        try:\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "            self.df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns),\n",
    "                                 pd.DataFrame(y_resampled, columns=[target_col])], axis=1)\n",
    "            logging.info(f\"Data balanced with new shape: {self.df.shape}\")\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"SMOTE ValueError: {e}\")\n",
    "            raise\n",
    "\n",
    "    def build_and_train_model(self, target_col, save_path):\n",
    "        logging.info(\"Building and training models...\")\n",
    "        X = self.df.drop(columns=[target_col])\n",
    "        y = self.df[target_col]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Logistic Regression\n",
    "        logistic_model = LogisticRegression(penalty='l2', solver='liblinear', random_state=42)\n",
    "        param_grid_logistic = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "        grid_logistic = GridSearchCV(logistic_model, param_grid_logistic, cv=5, scoring='accuracy')\n",
    "        grid_logistic.fit(X_train, y_train)\n",
    "        best_logistic = grid_logistic.best_estimator_\n",
    "        logging.info(f\"Best Logistic Regression Parameters: {grid_logistic.best_params_}\")\n",
    "\n",
    "        y_pred_logistic = best_logistic.predict(X_test)\n",
    "        logging.info(\"Logistic Regression Results:\")\n",
    "        logging.info(confusion_matrix(y_test, y_pred_logistic))\n",
    "        logging.info(classification_report(y_test, y_pred_logistic))\n",
    "\n",
    "        # Random Forest\n",
    "        rf_model = RandomForestClassifier(random_state=42)\n",
    "        param_grid_rf = {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, None], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]}\n",
    "        grid_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy')\n",
    "        grid_rf.fit(X_train, y_train)\n",
    "        best_rf = grid_rf.best_estimator_\n",
    "        logging.info(f\"Best Random Forest Parameters: {grid_rf.best_params_}\")\n",
    "\n",
    "        y_pred_rf = best_rf.predict(X_test)\n",
    "        logging.info(\"Random Forest Results:\")\n",
    "        logging.info(confusion_matrix(y_test, y_pred_rf))\n",
    "        logging.info(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "        # Save models and preprocessing objects\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        with open(os.path.join(save_path, 'logistic_model.pkl'), 'wb') as f:\n",
    "            pickle.dump(best_logistic, f)\n",
    "        with open(os.path.join(save_path, 'random_forest_model.pkl'), 'wb') as f:\n",
    "            pickle.dump(best_rf, f)\n",
    "        with open(os.path.join(save_path, 'scaler.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        with open(os.path.join(save_path, 'label_encoders.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.label_encoders, f)\n",
    "\n",
    "        logging.info(\"Models and preprocessing objects saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_file_path = r'D:\\GUVI_Projects\\My_Projects\\cleaned_crime.csv'\n",
    "    mappings_file_path = r'D:\\GUVI_Projects\\My_Projects\\cleaned_mappings_crime.csv'\n",
    "    save_path = r'D:\\GUVI_Projects\\My_Projects'\n",
    "\n",
    "    processor = DataProcessor(data_file_path, mappings_file_path)\n",
    "    processor.load_data()\n",
    "    processor.df = DataProcessor.reduce_memory(processor.df)  # Optimize memory\n",
    "    processor.preprocess_data()\n",
    "    processor.balance_data(target_col='Arrest')\n",
    "    processor.build_and_train_model(target_col='Arrest', save_path=save_path)\n",
    "    logging.info(\"Pipeline completed successfully.\")\n",
    "    \n",
    "# 2025-01-02 18:51:51,584 - INFO - Loading dataset using Polars...\n",
    "# 2025-01-02 18:51:53,434 - INFO - Dataset loaded successfully with shape: (1048575, 20)\n",
    "# 2025-01-02 18:51:54,653 - INFO - Dataset stratified sampled to shape: (948575, 20)\n",
    "# 2025-01-02 18:51:54,681 - INFO - Reducing memory usage...\n",
    "# 2025-01-02 18:51:54,687 - INFO - Initial memory usage: 139.31 MB\n",
    "# 2025-01-02 18:51:54,738 - INFO - Reduced memory usage: 94.99 MB (31.8% reduction)\n",
    "# 2025-01-02 18:51:54,738 - INFO - Preprocessing data...\n",
    "# 2025-01-02 18:51:54,738 - INFO - Cleaning unknown values in the dataset...\n",
    "# 2025-01-02 18:52:14,181 - INFO - Unknown values cleaned and data types corrected.\n",
    "# 2025-01-02 18:52:14,667 - INFO - Converting categorical columns using LabelEncoder...\n",
    "# 2025-01-02 18:52:22,601 - INFO - Standardizing numeric columns...\n",
    "# 2025-01-02 18:52:22,924 - INFO - Data preprocessing complete.\n",
    "# 2025-01-02 18:52:22,926 - INFO - Balancing data using SMOTE...\n",
    "# 2025-01-02 18:52:22,928 - INFO - Binarizing continuous target 'Arrest'...\n",
    "# 2025-01-02 18:53:57,558 - INFO - Data balanced with new shape: (1397218, 20)\n",
    "# 2025-01-02 18:53:57,575 - INFO - Building and training models...\n",
    "# 2025-01-02 19:02:19,449 - INFO - Best Logistic Regression Parameters: {'C': 0.1}\n",
    "# 2025-01-02 19:02:19,521 - INFO - Logistic Regression Results:\n",
    "# 2025-01-02 19:02:19,569 - INFO - [[88307 51338]\n",
    "#  [50310 89489]]\n",
    "# 2025-01-02 19:02:19,942 - INFO -               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.64      0.63      0.63    139645\n",
    "#            1       0.64      0.64      0.64    139799\n",
    "\n",
    "#     accuracy                           0.64    279444\n",
    "#    macro avg       0.64      0.64      0.64    279444\n",
    "# weighted avg       0.64      0.64      0.64    279444\n",
    "\n",
    "# 2025-01-03 13:49:18,800 - INFO - Best Random Forest Parameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
    "# 2025-01-03 13:49:31,620 - INFO - Random Forest Results:\n",
    "# 2025-01-03 13:49:31,646 - INFO - [[135074   4571]\n",
    "#  [ 18064 121735]]\n",
    "# 2025-01-03 13:49:31,961 - INFO -               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.88      0.97      0.92    139645\n",
    "#            1       0.96      0.87      0.91    139799\n",
    "\n",
    "#     accuracy                           0.92    279444\n",
    "#    macro avg       0.92      0.92      0.92    279444\n",
    "# weighted avg       0.92      0.92      0.92    279444\n",
    "\n",
    "# 2025-01-03 13:49:35,835 - INFO - Models and preprocessing objects saved.\n",
    "# 2025-01-03 13:49:35,983 - INFO - Pipeline completed successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a1954-de36-48c9-a6a4-d2a9fbd23209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, data_file_path, mappings_file_path):\n",
    "        self.data_file_path = data_file_path\n",
    "        self.mappings_file_path = mappings_file_path\n",
    "        self.df = None\n",
    "        self.scaler = None\n",
    "        self.label_encoders = {}\n",
    "\n",
    "    def reduce_memory(self, df):\n",
    "        logging.info(\"Reducing memory usage...\")\n",
    "        start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "        logging.info(f\"Initial memory usage: {start_mem:.2f} MB\")\n",
    "\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "            if col_type in ['int64', 'float64']:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if col_type == 'int64':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                elif col_type == 'float64':\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "        logging.info(f\"Reduced memory usage: {end_mem:.2f} MB ({((start_mem - end_mem) / start_mem) * 100:.1f}% reduction)\")\n",
    "        return df\n",
    "\n",
    "    def load_data(self):\n",
    "        logging.info(\"Loading dataset using Polars...\")\n",
    "        try:\n",
    "            polars_df = pl.read_csv(self.data_file_path)\n",
    "            self.df = polars_df.to_pandas()\n",
    "            logging.info(f\"Dataset loaded successfully with shape: {self.df.shape}\")\n",
    "\n",
    "            # Sample large dataset for development with stratified sampling\n",
    "            if self.df.shape[0] > 100000:\n",
    "                if 'Arrest' in self.df.columns:\n",
    "                    _, self.df = train_test_split(\n",
    "                        self.df,\n",
    "                        train_size=100000,\n",
    "                        stratify=self.df['Arrest'],\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    logging.info(f\"Dataset stratified sampled to shape: {self.df.shape}\")\n",
    "                else:\n",
    "                    logging.warning(\"Stratified sampling skipped as 'Arrest' column is not present. Performing random sampling.\")\n",
    "                    self.df = self.df.sample(n=100000, random_state=42)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clean_unknown_values(self):\n",
    "        logging.info(\"Cleaning unknown values in the dataset...\")\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtype == 'object':\n",
    "                self.df[col] = self.df[col].fillna('Unknown').astype(str)\n",
    "            else:\n",
    "                self.df[col] = self.df[col].apply(pd.to_numeric, errors='coerce').fillna(0).astype(float)\n",
    "        logging.info(\"Unknown values cleaned and data types corrected.\")\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        logging.info(\"Preprocessing data...\")\n",
    "        self.clean_unknown_values()\n",
    "\n",
    "        # Convert categorical columns to numeric using LabelEncoder\n",
    "        categorical_cols = self.df.select_dtypes(include=['object', 'category']).columns\n",
    "        if len(categorical_cols) > 0:\n",
    "            logging.info(\"Converting categorical columns using LabelEncoder...\")\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                self.df[col] = le.fit_transform(self.df[col])\n",
    "                self.label_encoders[col] = le\n",
    "\n",
    "        # Standardize numeric columns\n",
    "        numeric_cols = self.df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            logging.info(\"Standardizing numeric columns...\")\n",
    "            self.scaler = StandardScaler()\n",
    "            self.df[numeric_cols] = self.scaler.fit_transform(self.df[numeric_cols])\n",
    "\n",
    "        logging.info(\"Data preprocessing complete.\")\n",
    "\n",
    "    def balance_data(self, target_col):\n",
    "        logging.info(\"Balancing data using SMOTE...\")\n",
    "        try:\n",
    "            # Ensure the target column is binary\n",
    "            if self.df[target_col].dtype not in ['int64', 'int32', 'category']:\n",
    "                self.df[target_col] = self.df[target_col].astype(int)\n",
    "            \n",
    "            if self.df[target_col].nunique() > 2:\n",
    "                raise ValueError(f\"Target column '{target_col}' must be binary for SMOTE.\")\n",
    "\n",
    "            X = self.df.drop(columns=[target_col])\n",
    "            y = self.df[target_col]\n",
    "\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "            self.df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns),\n",
    "                                 pd.DataFrame(y_resampled, columns=[target_col])], axis=1)\n",
    "            logging.info(f\"Data balanced with new shape: {self.df.shape}\")\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"SMOTE ValueError: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error during SMOTE: {e}\")\n",
    "            raise\n",
    "\n",
    "    def build_and_train_model(self, target_col, save_path):\n",
    "        logging.info(\"Building and training models...\")\n",
    "        try:\n",
    "            X = self.df.drop(columns=[target_col])\n",
    "            y = self.df[target_col]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Neural Network Model with GridSearch for best parameters\n",
    "            param_grid = {\n",
    "                'hidden_layer_sizes': [(100,), (100, 50), (100, 100, 50)],\n",
    "                'max_iter': [200, 300, 400],\n",
    "                'activation': ['relu', 'tanh'],\n",
    "                'solver': ['adam']\n",
    "            }\n",
    "            nn_model = MLPClassifier(random_state=42)\n",
    "            grid_search = GridSearchCV(estimator=nn_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            best_nn = grid_search.best_estimator_\n",
    "            logging.info(\"Best Neural Network Parameters:\")\n",
    "            pprint(grid_search.best_params_)\n",
    "\n",
    "            y_pred_nn = best_nn.predict(X_test)\n",
    "            logging.info(\"Neural Network Results:\")\n",
    "            logging.info(confusion_matrix(y_test, y_pred_nn))\n",
    "            logging.info(classification_report(y_test, y_pred_nn))\n",
    "\n",
    "            # Save Neural Network model and preprocessing objects\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            with open(os.path.join(save_path, 'nn_model.pkl'), 'wb') as f:\n",
    "                pickle.dump(best_nn, f)\n",
    "            with open(os.path.join(save_path, 'scaler.pkl'), 'wb') as f:\n",
    "                pickle.dump(self.scaler, f)\n",
    "            with open(os.path.join(save_path, 'label_encoders.pkl'), 'wb') as f:\n",
    "                pickle.dump(self.label_encoders, f)\n",
    "\n",
    "            logging.info(\"Model and preprocessing objects saved.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during model training: {e}\")\n",
    "            raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_file_path = r'D:\\GUVI_Projects\\My_Projects\\cleaned_crime.csv'\n",
    "    mappings_file_path = r'D:\\GUVI_Projects\\My_Projects\\cleaned_mappings_crime.csv'\n",
    "    save_path = r'D:\\GUVI_Projects\\My_Projects'\n",
    "\n",
    "    processor = DataProcessor(data_file_path, mappings_file_path)\n",
    "    processor.load_data()\n",
    "    processor.df = processor.reduce_memory(processor.df)  # Use instance method\n",
    "    processor.preprocess_data()\n",
    "    processor.balance_data(target_col='Arrest')\n",
    "    processor.build_and_train_model(target_col='Arrest', save_path=save_path)\n",
    "    logging.info(\"Pipeline completed successfully.\")\n",
    "    \n",
    "# 2025-01-09 15:36:29,168 - INFO - Loading dataset using Polars...\n",
    "# 2025-01-09 15:36:31,015 - INFO - Dataset loaded successfully with shape: (1048575, 20)\n",
    "# 2025-01-09 15:36:32,037 - INFO - Dataset stratified sampled to shape: (948575, 20)\n",
    "# 2025-01-09 15:36:32,055 - INFO - Reducing memory usage...\n",
    "# 2025-01-09 15:36:32,061 - INFO - Initial memory usage: 139.31 MB\n",
    "# 2025-01-09 15:36:32,119 - INFO - Reduced memory usage: 94.99 MB (31.8% reduction)\n",
    "# 2025-01-09 15:36:32,120 - INFO - Preprocessing data...\n",
    "# 2025-01-09 15:36:32,120 - INFO - Cleaning unknown values in the dataset...\n",
    "# 2025-01-09 15:36:52,393 - INFO - Unknown values cleaned and data types corrected.\n",
    "# 2025-01-09 15:36:52,959 - INFO - Converting categorical columns using LabelEncoder...\n",
    "# 2025-01-09 15:37:00,619 - INFO - Standardizing numeric columns...\n",
    "# 2025-01-09 15:37:00,890 - INFO - Data preprocessing complete.\n",
    "# 2025-01-09 15:37:00,891 - INFO - Balancing data using SMOTE...\n",
    "# 2025-01-09 15:38:21,374 - INFO - Data balanced with new shape: (1397218, 20)\n",
    "# 2025-01-09 15:38:21,399 - INFO - Building and training models...\n",
    "# 2025-01-09 21:44:13,616 - INFO - Best Neural Network Parameters:\n",
    "# {'activation': 'tanh',\n",
    "#  'hidden_layer_sizes': (100, 100, 50),\n",
    "#  'max_iter': 200,\n",
    "#  'solver': 'adam'}\n",
    "# 2025-01-09 21:44:14,850 - INFO - Neural Network Results:\n",
    "# 2025-01-09 21:44:14,906 - INFO - [[79345 60300]\n",
    "#  [66876 72923]]\n",
    "# 2025-01-09 21:44:15,254 - INFO -               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.54      0.57      0.56    139645\n",
    "#            1       0.55      0.52      0.53    139799\n",
    "\n",
    "#     accuracy                           0.54    279444\n",
    "#    macro avg       0.55      0.54      0.54    279444\n",
    "# weighted avg       0.55      0.54      0.54    279444\n",
    "\n",
    "# 2025-01-09 21:44:16,265 - INFO - Model and preprocessing objects saved.\n",
    "# 2025-01-09 21:44:16,289 - INFO - Pipeline completed successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c8d83-5aa5-477a-a888-98e419560dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
